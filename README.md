# Portfolio 1 - Yelp
This repository comprises the necessary files for the Portfolio tasks assigned in COMP2200/6200 S1 2023. 

The dataset (Yelp_Portfolio1_Input.csv) can be accessed via the link provided at https://github.com/COMP2200-S1-2023/portfolio-part-1-dataset/releases/download/portfolio-dataset-p1/Yelp_Portfolio1_Input.csv

In Portfolio 1, the Yelp dataset is provided to analyse patterns and trends in consumer behaviour, business performance, and geographic locations. Data cleaning is carried out at the beginning stage since there are missing or misleading data. After cleaning up the dataset, it is decided to select 10 random cities to have analysis based on data from those 10 cities. From the 10 sample cities, the number of unique businesses in each city is explored and how many unique users have rated the business is realised. At last, more data exploration is implemented with visualisation to see if there is any hidden patterns or relationship between the features.

# Portfolio 2 - Yelp

Portfolio 2 works on Yelp dataset and in continuation of Portfolio 1 where using the output from there. In Portfolio 2, it goes through data processing and apply Linear Regression Model to train and validate the model's accuracy. At first, each variable in `business_categories` is replaced with its last category value to facilitate superior categorization. Then, reserve unique business categories that appear fewer than 200 times. After that, boxplot is used to check outliers of `useful` column and extremely active users, and the outliers are removed accordingly. Once the data processing is completed, Linear Regression Model is built based on the training and test set. The targets are relationship between `useful` and `stars`, and relationship between `business_review_count` and `stars`. R squared and MSE methods are used to see whether the model is a good fit or not. Finally, analysis of the correlation between `business_review_count` and `useful` variables with `stars` is carried out.

# Portfolio 3 - LendingClub Dataset

Portfolio 3 is about the dataset of Lending Club that contains the home loan records. The purpose of the portfolio is to classify whether a homeland case should be approved based on borrower's features. The analysis starts with data cleaning. Abnormal data points are removed and missing values are filled with median values. Then, we discuss about whether to normalise the features through examining the overall descriptive statistics. After the discussion, heatmap is created to check the correlation between features and only features with positive correlations with `credit.policy` are retained. Before the implementation of Logistic Regression Model, categorical feature `purpose` is converted to number with OneHotEncoder such that the model can handle it. The dataset is then split into training, validation and test set. Standardization is implemented to normalise the dataset as the result of the previous discussion. Accordingly, the Logistic Regression Model is fitted with normalised dataset. At last, a 9-fold cross validation is used to check the model accuracy.

# Portfolio 4 - Price prediction of ridesharing service in Boston

The dataset can be accessed via the link provided at https://www.kaggle.com/datasets/brllrb/uber-and-lyft-dataset-boston-ma?resource=download

Portfolio 4 is about the price prediction of ridesharing service in Boston. The data is collected from 11-26-2018 to 12-18-2018. At the beginning, irrelevant features are removed and only 21 features are remained. `temperature` is converted to Celsius and `weekday` information is obtained from the date provided. Since there are many missing values, the data with missing values are removed from the dataset. Consequently, we explore the data to see if there are any patterns or features that could be related to price. Also, the `price` feature is checked to ensure there is no outlier. Boxplot is used to identify the outliers of `price` and remove them accordingly.  During data processing, more features are removed from the dataset. The features are removed based on the correlation between the features and `price` through heatmap. Low correlation features are removed and retain high correlation features. Finally, the analysis uses Linear Regression, KNN, and Gaussian Naive Bayes Model to see which model performs the best. 
